# Distributed Movie Ticket Booking System

A fault-tolerant distributed movie ticket booking system implementing the Raft consensus protocol with gRPC communication and LLM-powered customer support.

## ğŸ¯ Project Overview

This system implements a distributed ticket booking platform with:
- **Raft Consensus**: Ensures strong consistency across multiple nodes
- **gRPC Communication**: High-performance RPC framework for inter-service communication
- **LLM Integration**: AI-powered customer support chatbot using DistilGPT2
- **Fault Tolerance**: Automatic leader election and recovery from node failures
- **Concurrency Control**: Prevents overbooking with distributed locks

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ gRPC
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Application Servers (3 nodes)       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Node1  â”‚ â”‚  Node2  â”‚ â”‚  Node3  â”‚  â”‚
â”‚  â”‚ LEADER  â”‚ â”‚FOLLOWER â”‚ â”‚FOLLOWER â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€Raft Consensusâ”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ gRPC
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ LLM Server  â”‚
          â”‚ (DistilGPT2)â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- Python 3.9+
- pip
- Virtual environment support
- 4GB RAM minimum (for LLM model)
- Linux/macOS (tested on Ubuntu)

## ğŸš€ Quick Start

### 1. Setup

```bash
# Clone the repository
git clone <repository-url>
cd distributed_ticket_booking

# Run complete setup
chmod +x complete_setup.sh
./complete_setup.sh
```

This will:
- Create directory structure
- Set up Python virtual environment
- Install all dependencies
- Generate gRPC protobuf files
- Fix import paths

### 2. Start the System

```bash
# Start all servers (LLM + 3 Raft nodes)
chmod +x start_system.sh
./start_system.sh
```

**Wait 10-15 seconds** for:
- LLM model to load
- Raft leader election to complete
- System initialization

### 3. Start a Client

In a **new terminal**:

```bash
source venv/bin/activate
./start_client.sh
```

## ğŸ“ Usage Guide

### Login Credentials

Default users:
- `user1` / `password1`
- `user2` / `password2`
- `admin` / `admin123`

### Client Menu Options

1. **ğŸ¬ View Movies** - See available movies with pricing and seats
2. **ğŸ« Book Tickets** - Book seats for a movie
3. **ğŸ“‹ View My Bookings** - See your booking history
4. **âŒ Cancel Booking** - Cancel a booking (refund processed)
5. **ğŸ¤– Ask AI Assistant** - Get help from the LLM chatbot
6. **ğŸšª Logout** - End your session

### Booking Flow

```bash
# 1. Login
Username: user1
Password: password1

# 2. View Movies
> Select option 1

# 3. Book Tickets
> Select option 2
> Enter Movie ID: movie1
> Enter seat numbers: 50,51,52

# 4. Process Payment
> Process payment now? y
> Payment method: card
```

## ğŸ”§ System Components

### Application Servers (Ports: 50051, 50052, 50053)
- Handle client requests
- Implement Raft consensus
- Manage ticket booking state
- Communicate with LLM server

### Raft Nodes (Ports: 50061, 50062, 50063)
- Leader election
- Log replication
- Failure detection
- State machine consistency

### LLM Server (Port: 50060)
- Customer support chatbot
- FAQ responses
- Context-aware assistance

## ğŸ§ª Testing

### Run All Tests

```bash
# Concurrent booking test (prevent overbooking)
python3 test_concurrent_bookings.py

# Raft consensus test
python3 test_raft_consensus.py

# Quick functionality test
python3 quick_test.py
```

### Health Check

```bash
# Check server status
./diagnose.sh

# Check which node is leader
python3 check_leader.py

# Monitor logs
tail -f logs/app_server_1.log
```

### Test Scenarios

**Test 1: Concurrent Booking Prevention**
```bash
# Multiple clients try to book same seats simultaneously
# Expected: Only 1 succeeds (no overbooking)
python3 test_concurrent_bookings.py
```

**Test 2: Leader Failure & Re-election**
```bash
# 1. Start system
./start_system.sh

# 2. Check leader
grep "LEADER" logs/app_server_*.log

# 3. Kill leader process
pkill -f "application_server.py.*node1"

# 4. Verify new leader elected (within 10 seconds)
grep "LEADER" logs/app_server_*.log
```

## ğŸ“Š Monitoring

### View Logs

```bash
# All logs
./check_all_logs.sh

# Specific server
tail -f logs/app_server_1.log
tail -f logs/llm_server.log

# Monitor Raft activity
grep -h "LEADER\|FOLLOWER\|election" logs/app_server_*.log
```

### Check System Status

```bash
# Process status
ps aux | grep -E "application_server|llm_server"

# Port status
netstat -tuln | grep -E "5005[123]|5006[0123]"

# Health check
python3 scripts/health_check.py
```

## ğŸ› ï¸ Configuration

Edit `.env` file for configuration:

```bash
# Server Ports
APP_SERVER_1_PORT=50051
APP_SERVER_2_PORT=50052
APP_SERVER_3_PORT=50053
LLM_SERVER_PORT=50060

# Raft Ports
RAFT_PORT_1=50061
RAFT_PORT_2=50062
RAFT_PORT_3=50063

# Raft Timing (seconds)
ELECTION_TIMEOUT_MIN=5.0
ELECTION_TIMEOUT_MAX=10.0
HEARTBEAT_INTERVAL=2.0

# Authentication
JWT_SECRET_KEY=dcdf397aeeef9d64d95b97ecb0691aaf
TOKEN_EXPIRY_HOURS=24

# LLM Settings
LLM_MODEL=distilgpt2
LLM_MAX_LENGTH=200
LLM_TEMPERATURE=0.7
```

## ğŸ› Troubleshooting

### System Won't Start

```bash
# Check if ports are in use
netstat -tuln | grep -E "5005[123]|5006"

# Kill existing processes
pkill -f "application_server.py"
pkill -f "llm_server.py"

# Clean logs and restart
rm -f logs/*.log
./start_system.sh
```

### Raft Not Electing Leader

```bash
# Check Raft connectivity
python3 test_raft_ports.py

# Verify all nodes started
ps aux | grep application_server

# Check logs for errors
grep -i "error\|exception" logs/app_server_*.log
```

### Client Connection Failed

```bash
# Check if servers are running
./diagnose.sh

# Verify leader exists
python3 check_leader.py

# Try connecting to specific server
# Edit client.py to use: ['localhost:50051']
```

### LLM Not Responding

```bash
# Check LLM server status
tail -f logs/llm_server.log

# Verify model loaded
grep "Model loaded" logs/llm_server.log

# Restart LLM server only
pkill -f llm_server.py
cd src/servers
python3 llm_server.py --port 50060 > ../../logs/llm_server.log 2>&1 &
```

## ğŸ“ Project Structure

```
distributed_ticket_booking/
â”œâ”€â”€ protos/                     # Protocol buffer definitions
â”‚   â”œâ”€â”€ ticket_booking.proto    # Main service API
â”‚   â”œâ”€â”€ raft.proto              # Raft consensus protocol
â”‚   â””â”€â”€ llm_service.proto       # LLM service API
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ client/                 # Client implementation
â”‚   â”‚   â”œâ”€â”€ client.py           # Interactive CLI client
â”‚   â”‚   â””â”€â”€ *_pb2*.py           # Generated protobuf files
â”‚   â”œâ”€â”€ servers/                # Server implementations
â”‚   â”‚   â”œâ”€â”€ application_server.py  # Main app server with Raft
â”‚   â”‚   â”œâ”€â”€ llm_server.py       # LLM inference server
â”‚   â”‚   â””â”€â”€ *_pb2*.py           # Generated protobuf files
â”‚   â”œâ”€â”€ raft/                   # Raft consensus implementation
â”‚   â”‚   â”œâ”€â”€ raft_node.py        # Raft algorithm core
â”‚   â”‚   â”œâ”€â”€ state_machine.py   # Application state machine
â”‚   â”‚   â””â”€â”€ *_pb2*.py           # Generated protobuf files
â”‚   â””â”€â”€ utils/                  # Utility modules
â”‚       â””â”€â”€ auth.py             # JWT authentication
â”œâ”€â”€ logs/                       # Server logs
â”œâ”€â”€ .env                        # Configuration file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ start_system.sh             # Start all servers
â”œâ”€â”€ start_client.sh             # Start client
â”œâ”€â”€ complete_setup.sh           # Complete setup script
â””â”€â”€ README.md                   # This file
```

## ğŸ” Key Features Demonstrated

### 1. Raft Consensus
- âœ… Leader election
- âœ… Log replication
- âœ… Failure detection and recovery
- âœ… Strong consistency guarantees

### 2. gRPC Communication
- âœ… Client-Server RPC
- âœ… Server-Server internal RPC
- âœ… Protobuf message serialization
- âœ… Efficient binary protocol

### 3. LLM Integration
- âœ… Domain-specific responses
- âœ… Rule-based FAQ system
- âœ… Context-aware assistance
- âœ… CPU-optimized inference

### 4. Distributed Features
- âœ… Concurrency control
- âœ… Overbooking prevention
- âœ… Automatic failover
- âœ… State replication

## ğŸ“š Assignment Requirements Checklist

- âœ… gRPC communication for all inter-service messaging
- âœ… Raft consensus for distributed consistency
- âœ… Leader election mechanism
- âœ… Log replication across nodes
- âœ… Failure detection and recovery
- âœ… Real-time seat reservation with concurrency control
- âœ… Overbooking prevention
- âœ… Payment processing (mock)
- âœ… Booking state replication using Raft
- âœ… Fault tolerance on leader/follower failure
- âœ… LLM customer support chatbot
- âœ… 4-5 node system (1 LLM + 3 App servers)
- âœ… Python implementation
- âœ… Proper authentication (JWT tokens)

## ğŸ¥ Demo Scenarios

### Scenario 1: Normal Operation
1. Start system
2. Login as user1
3. View available movies
4. Book tickets for movie1, seats [50,51,52]
5. View booking confirmation
6. Ask LLM: "How do I cancel my booking?"

### Scenario 2: Concurrent Booking
1. Start 5 clients simultaneously
2. All try to book same seats [1,2,3]
3. Only 1 succeeds (others get "seats unavailable")
4. Demonstrates race condition prevention

### Scenario 3: Leader Failure
1. System running with node1 as leader
2. Kill node1 process
3. Within 10 seconds, node2 or node3 becomes new leader
4. System continues accepting requests
5. Demonstrates fault tolerance

## ğŸ“– References

- [Raft Consensus Paper](https://raft.github.io/raft.pdf) - "In Search of an Understandable Consensus Algorithm"
- [gRPC Documentation](https://grpc.io/docs/)
- [Protocol Buffers](https://developers.google.com/protocol-buffers)

## ğŸ‘¥ Team Information

- Team Size: 3 members (as per assignment requirements)
- Project: Distributed Movie Ticket Booking System (Option A)
- Course: Advanced Operating Systems (CS G623)
- Semester: First Semester 2025-26

## ğŸ“§ Support

For issues or questions:
1. Check logs: `tail -f logs/*.log`
2. Run diagnostics: `./diagnose.sh`
3. Check health: `python3 scripts/health_check.py`
4. Review troubleshooting section above

## âš ï¸ Important Notes

1. **First Run**: Initial LLM model download takes 1-2 minutes
2. **Ports**: Ensure ports 50051-50053 and 50060-50063 are free
3. **Memory**: LLM requires ~2GB RAM
4. **Timing**: Allow 10-15 seconds for system initialization
5. **Leader Election**: First election takes 5-10 seconds

## ğŸ“ Learning Outcomes

This project demonstrates:
- Distributed consensus algorithms (Raft)
- RPC communication (gRPC)
- Fault-tolerant system design
- Concurrent programming
- State machine replication
- LLM integration in distributed systems

---

**Status**: âœ… Fully Functional | **Last Updated**: November 2025 | **Version**: 1.0.0